{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0cdc2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-05 10:57:23.693597: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from keras_preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99639ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一身遶千山，遠作行路人。未遂東吳歸，暫出西京塵。仲宣荆州客，今余竟陵賓。往蹟雖不同，託意皆有因。商嶺莓苔滑，石坂上下頻。江漢沙泥潔，永日光景新。獨淚起殘夜，孤吟望初晨。驅馳竟何事，章句依深仁。\n",
      "楚山爭蔽虧，日月無全輝。楚路饒迴惑，旅人有迷歸。騏驥思北首，鷓鴣願南飛。我懷京洛遊，未厭風塵衣。\n",
      "衆䖟聚病馬，流血不得行。後路起夜色，前山聞虎聲。此時遊子心，百尺風中旌。\n",
      "分拙多感激，久遊遵長途。經過湘水源，懷古方踟躕。舊稱楚靈均，此處殞忠軀。側聆故老言，遂得旌賢愚。名參君子場，行爲小人儒。騷文衒貞亮，體物情崎嶇。三黜有慍色，即非賢哲模。五十爵高秩，謬膺從大夫。胸襟積憂愁，容鬢復彫枯。死爲不弔鬼，生作猜謗徒。吟澤潔其身，忠節寧見輸。懷沙滅其性，孝行焉能俱。且聞善稱君，一何善自殊。且聞過稱己，一何過不渝。悠哉風土人，角黍投川隅。相傳歷千祀，哀悼延八區。如今聖明朝，養育無羈孤。君臣逸雍熙，德化盈紛敷。巾車徇前侶，白日猶昆吾。寄君臣子心，戒此真良圖。\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob(\"/home/imcurie/ml/chinese-poetry/全唐诗/poet.tang.*.json\")\n",
    "write_dir = \"/home/imcurie/ml/neuron-network-learning/data/poetry\"\n",
    "write_file_name = \"poetry.txt\"\n",
    "full_path = os.path.join(write_dir, write_file_name)\n",
    "\n",
    "if not os.path.exists(full_path):\n",
    "    for file in files:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f_read:\n",
    "            json_datas = json.load(f_read)\n",
    "            for json_data in json_datas:\n",
    "                with open(full_path, \"a\", encoding=\"utf-8\") as f_write:\n",
    "                    poetry = ''\n",
    "                    for paragraph in json_data[\"paragraphs\"]:\n",
    "                        poetry += paragraph\n",
    "                    f_write.write(poetry + \"\\n\")\n",
    "\n",
    "datas = []\n",
    "with open(full_path, 'r', encoding=\"utf-8\") as f:\n",
    "     while line := f.readline():\n",
    "        datas.append(line.strip('\\n'))\n",
    "        \n",
    "for i in range(4):\n",
    "    print(datas[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad1a9a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> 一 身 遶 千 山 ， 遠 作 行 路 人 。 未 遂 東 吳 歸 ， 暫 出 西 京 塵 。 仲 宣 荆 州 客 ， 今 余 竟 陵 賓 。 往 蹟 雖 不 同 ， 託 意 皆 有 因 。 商 嶺 莓 苔 滑 ， 石 坂 上 下 頻 。 江 漢 沙 泥 潔 ， 永 日 光 景 新 。 獨 淚 起 殘 夜 ， 孤 吟 望 初 晨 。 驅 馳 竟 何 事 ， 章 句 依 深 仁 。 <EOS>\n",
      "<SOS> 楚 山 爭 蔽 虧 ， 日 月 無 全 輝 。 楚 路 饒 迴 惑 ， 旅 人 有 迷 歸 。 騏 驥 思 北 首 ， 鷓 鴣 願 南 飛 。 我 懷 京 洛 遊 ， 未 厭 風 塵 衣 。 <EOS>\n",
      "<SOS> 衆 䖟 聚 病 馬 ， 流 血 不 得 行 。 後 路 起 夜 色 ， 前 山 聞 虎 聲 。 此 時 遊 子 心 ， 百 尺 風 中 旌 。 <EOS>\n",
      "<SOS> 分 拙 多 感 激 ， 久 遊 遵 長 途 。 經 過 湘 水 源 ， 懷 古 方 踟 躕 。 舊 稱 楚 靈 均 ， 此 處 殞 忠 軀 。 側 聆 故 老 言 ， 遂 得 旌 賢 愚 。 名 參 君 子 場 ， 行 爲 小 人 儒 。 騷 文 衒 貞 亮 ， 體 物 情 崎 嶇 。 三 黜 有 慍 色 ， 即 非 賢 哲 模 。 五 十 爵 高 秩 ， 謬 膺 從 大 夫 。 胸 襟 積 憂 愁 ， 容 鬢 復 彫 枯 。 死 爲 不 弔 鬼 ， 生 作 猜 謗 徒 。 吟 澤 潔 其 身 ， 忠 節 寧 見 輸 。 懷 沙 滅 其 性 ， 孝 行 焉 能 俱 。 且 聞 善 稱 君 ， 一 何 善 自 殊 。 且 聞 過 稱 己 ， 一 何 過 不 渝 。 悠 哉 風 土 人 ， 角 黍 投 川 隅 。 相 傳 歷 千 祀 ， 哀 悼 延 八 區 。 如 今 聖 明 朝 ， 養 育 無 羈 孤 。 君 臣 逸 雍 熙 ， 德 化 盈 紛 敷 。 巾 車 徇 前 侶 ， 白 日 猶 昆 吾 。 寄 君 臣 子 心 ， 戒 此 真 良 圖 。 <EOS>\n"
     ]
    }
   ],
   "source": [
    "poetry_num = len(datas)\n",
    "for i in range(poetry_num):\n",
    "    datas[i] = ' '.join(datas[i])\n",
    "\n",
    "for i in range(poetry_num):\n",
    "    datas[i] = '<SOS> ' + datas[i] + ' <EOS>'\n",
    "    \n",
    "for i in range(4):\n",
    "    print(datas[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2c3f9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('，', 1)\n",
      "('。', 2)\n",
      "('<sos>', 3)\n",
      "('<eos>', 4)\n",
      "('不', 5)\n",
      "('人', 6)\n",
      "('一', 7)\n",
      "('無', 8)\n",
      "('山', 9)\n",
      "('風', 10)\n",
      "===================\n",
      "('<sos>', 57607)\n",
      "('一', 20277)\n",
      "('身', 5325)\n",
      "('遶', 748)\n",
      "('千', 6922)\n",
      "('山', 18212)\n",
      "('，', 268814)\n",
      "('遠', 6429)\n",
      "('作', 8982)\n",
      "('行', 9130)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=10000, char_level=False, filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(datas)\n",
    "\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n",
    "\n",
    "## 使用word_index属性查看每个词对应的编码\n",
    "## 使用word_counts属性查看每个词对应的频数\n",
    "for ii, iterm in enumerate(tokenizer.word_index.items()):\n",
    "    if ii < 10:\n",
    "        print(iterm)\n",
    "    else:\n",
    "        break\n",
    "print(\"===================\")  \n",
    "for ii, iterm in enumerate(tokenizer.word_counts.items()):\n",
    "    if ii < 10:\n",
    "        print(iterm)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39709467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照空格进行分词\n",
    "datas_split = []\n",
    "for i in range(poetry_num):\n",
    "    datas_split.append(datas[i].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6068dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 57607/57607 [00:00<00:00, 167340.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['潔', '，', '永', '日', '光', '景', '新', '。', '獨', '淚'] \n",
      " ['起', '殘', '夜', '，', '孤', '吟', '望', '初', '晨', '。'] \n",
      " ['殘', '夜', '，', '孤', '吟', '望', '初', '晨', '。', '驅'] \n",
      "\n",
      "\n",
      "['輝', '。', '楚', '路', '饒', '迴', '惑', '，', '旅', '人'] \n",
      " ['有', '迷', '歸', '。', '騏', '驥', '思', '北', '首', '，'] \n",
      " ['迷', '歸', '。', '騏', '驥', '思', '北', '首', '，', '鷓'] \n",
      "\n",
      "\n",
      "['得', '行', '。', '後', '路', '起', '夜', '色', '，', '前'] \n",
      " ['山', '聞', '虎', '聲', '。', '此', '時', '遊', '子', '心'] \n",
      " ['聞', '虎', '聲', '。', '此', '時', '遊', '子', '心', '，'] \n",
      "\n",
      "\n",
      "['過', '湘', '水', '源', '，', '懷', '古', '方', '踟', '躕'] \n",
      " ['。', '舊', '稱', '楚', '靈', '均', '，', '此', '處', '殞'] \n",
      " ['舊', '稱', '楚', '靈', '均', '，', '此', '處', '殞', '忠'] \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x_seq_ls = []\n",
    "y_seq_ls = []\n",
    "decoder_output_ls = []\n",
    "\n",
    "for i in tqdm(range(poetry_num)):\n",
    "    \n",
    "    if len(datas_split[i]) - 21 <= 0:\n",
    "        continue\n",
    "    \n",
    "    start = random.randint(0, len(datas_split[i]) - 11)\n",
    "    end = start + 10\n",
    "\n",
    "    x_seq_ls.append(datas_split[i][start:end])\n",
    "    y_seq_ls.append(datas_split[i][end:end+10])\n",
    "    decoder_output_ls.append(datas_split[i][end+1:end+11])\n",
    "\n",
    "for i in range(4):\n",
    "    print(x_seq_ls[i],'\\n',y_seq_ls[i],'\\n',decoder_output_ls[i],'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d5fd79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1422, 1, 565, 11, 118, 299, 98, 2, 125, 341] \n",
      " [185, 280, 54, 1, 214, 207, 135, 187, 712, 2] \n",
      " [280, 54, 1, 214, 207, 135, 187, 712, 2, 845] \n",
      "\n",
      "\n",
      "[888, 2, 309, 84, 1276, 317, 2393, 1, 849, 6] \n",
      " [12, 591, 39, 2, 3283, 2232, 101, 188, 279, 1] \n",
      " [591, 39, 2, 3283, 2232, 101, 188, 279, 1, 2404] \n",
      "\n",
      "\n",
      "[47, 40, 2, 158, 84, 185, 54, 107, 1, 63] \n",
      " [9, 124, 608, 71, 2, 31, 19, 167, 61, 29] \n",
      " [124, 608, 71, 2, 31, 19, 167, 61, 29, 1] \n",
      "\n",
      "\n",
      "[178, 573, 21, 824, 1, 342, 132, 142, 2551, 2526] \n",
      " [2, 169, 645, 309, 308, 1190, 1, 31, 62, 4551] \n",
      " [169, 645, 309, 308, 1190, 1, 31, 62, 4551, 1293] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_token = tokenizer.texts_to_sequences(x_seq_ls)\n",
    "y_token = tokenizer.texts_to_sequences(y_seq_ls)\n",
    "decoder_output_token = tokenizer.texts_to_sequences(decoder_output_ls)\n",
    "\n",
    "for i in range(4):\n",
    "    print(x_token[i],'\\n',y_token[i],'\\n',decoder_output_token[i],'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f652b7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1422    1  565   11  118  299   98    2  125  341] \n",
      " [185 280  54   1 214 207 135 187 712   2] \n",
      "\n",
      "\n",
      "[ 888    2  309   84 1276  317 2393    1  849    6] \n",
      " [  12  591   39    2 3283 2232  101  188  279    1] \n",
      "\n",
      "\n",
      "[ 47  40   2 158  84 185  54 107   1  63] \n",
      " [  9 124 608  71   2  31  19 167  61  29] \n",
      "\n",
      "\n",
      "[ 178  573   21  824    1  342  132  142 2551 2526] \n",
      " [   2  169  645  309  308 1190    1   31   62 4551] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 导入补零需要的padding_seq\n",
    "x_mat = pad_sequences(x_token, maxlen=10, padding='post', truncating='post')\n",
    "y_mat = pad_sequences(y_token, maxlen=10, padding='post', truncating='post')\n",
    "decoder_output_mat = pad_sequences(decoder_output_token, maxlen=10, padding='post', truncating='post')\n",
    "decoder_output_mat = decoder_output_mat.reshape(decoder_output_mat.shape[0], decoder_output_mat.shape[1], 1)\n",
    "\n",
    "for i in range(4):\n",
    "    print(x_mat[i],'\\n',y_mat[i],'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6441716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 894    1  454   22  339 2022  155    2   31   16] \n",
      " [  25   36  561    1  985  756  171   57 1037    2] \n",
      "\n",
      "\n",
      "[2587  541  565    1  196  398   16  369    2 2325] \n",
      " [ 845 1399 4292    1   14  916 2159 2007    2   41] \n",
      "\n",
      "\n",
      "[ 285  135   62    1 1434  734  501 4032  327  126] \n",
      " [86  2  4  0  0  0  0  0  0  0] \n",
      "\n",
      "\n",
      "[   7 3209    1   83   29  146 1023   83    8  146] \n",
      " [   2   77 1149  100   23 1818  741  671    1  869] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test, decoder_output_train, decoder_output_test = train_test_split(x_mat, y_mat, decoder_output_mat, test_size=0.3, random_state=0)\n",
    "\n",
    "for i in range(4):\n",
    "    print(x_train[i],'\\n',y_train[i],'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "713fb53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 894,    1,  454,   22,  339, 2022,  155,    2,   31,   16],\n",
      "       dtype=torch.int32) \n",
      " tensor([  25,   36,  561,    1,  985,  756,  171,   57, 1037,    2],\n",
      "       dtype=torch.int32) \n",
      "\n",
      "\n",
      "tensor([2587,  541,  565,    1,  196,  398,   16,  369,    2, 2325],\n",
      "       dtype=torch.int32) \n",
      " tensor([ 845, 1399, 4292,    1,   14,  916, 2159, 2007,    2,   41],\n",
      "       dtype=torch.int32) \n",
      "\n",
      "\n",
      "tensor([ 285,  135,   62,    1, 1434,  734,  501, 4032,  327,  126],\n",
      "       dtype=torch.int32) \n",
      " tensor([86,  2,  4,  0,  0,  0,  0,  0,  0,  0], dtype=torch.int32) \n",
      "\n",
      "\n",
      "tensor([   7, 3209,    1,   83,   29,  146, 1023,   83,    8,  146],\n",
      "       dtype=torch.int32) \n",
      " tensor([   2,   77, 1149,  100,   23, 1818,  741,  671,    1,  869],\n",
      "       dtype=torch.int32) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train_tensor = torch.tensor(x_train)\n",
    "x_test_tensor = torch.tensor(x_test)\n",
    "y_train_tensor = torch.tensor(y_train)\n",
    "y_test_tensor = torch.tensor(y_test)\n",
    "decoder_output_train_tensor = torch.tensor(decoder_output_train).long()\n",
    "decoder_output_test_tensor = torch.tensor(decoder_output_test).long()\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    print(x_train_tensor[i],'\\n',y_train_tensor[i],'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dc5743d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4, 0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SOS_token = tokenizer.word_index['<sos>']\n",
    "EOS_token = tokenizer.word_index['<eos>']\n",
    "PAD_token = tokenizer.word_index['<pad>']\n",
    "SOS_token,EOS_token,PAD_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bf923cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # 初始化Shape为(max_len, d_model)的PE (positional encoding)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # 初始化一个tensor [[0, 1, 2, 3, ...]]\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        # 这里就是sin和cos括号中的内容，通过e和ln进行了变换\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        # 计算PE(pos, 2i)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # 计算PE(pos, 2i+1)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # 为了方便计算，在最外面在unsqueeze出一个batch\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # 如果一个参数不参与梯度下降，但又希望保存model的时候将其保存下来\n",
    "        # 这个时候就可以用register_buffer\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x 为embedding后的inputs, 例如(1, 7, 128), batch size为1, 7个单词, 单词维度为128\n",
    "        \"\"\"\n",
    "        # 将x和positional encoding相加，同时将它们移到相同的设备上\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41f44b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoetTaskModel(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=128):\n",
    "        super(PoetTaskModel, self).__init__()\n",
    "\n",
    "        # 词典数为10000\n",
    "        self.embedding = nn.Embedding(num_embeddings=10000, embedding_dim=128)\n",
    "        self.transformer = nn.Transformer(d_model=128, nhead = 2 ,num_encoder_layers=3, num_decoder_layers=3, dim_feedforward=128, batch_first=True)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout=0)\n",
    "\n",
    "        # 定义最后的线性层，这里并没有用Softmax，因为没必要。\n",
    "        # 因为后面的CrossEntropyLoss中自带了\n",
    "        self.predictor = nn.Linear(128, 10000)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "\n",
    "        # 生成mask\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size()[-1])\n",
    "        src_key_padding_mask = PoetTaskModel.get_key_padding_mask(src)\n",
    "        tgt_key_padding_mask = PoetTaskModel.get_key_padding_mask(tgt)\n",
    "\n",
    "        src = self.embedding(src)\n",
    "        tgt = self.embedding(tgt)\n",
    "        src = self.positional_encoding(src)\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "\n",
    "        # 将准备好的数据送给transformer\n",
    "        out = self.transformer(src, tgt,\n",
    "                               tgt_mask=tgt_mask,\n",
    "                               src_key_padding_mask=src_key_padding_mask,\n",
    "                               tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "\n",
    "        \"\"\"\n",
    "        这里直接返回transformer的结果。因为训练和推理时的行为不一样，\n",
    "        所以在该模型外再进行线性层的预测。\n",
    "        \"\"\"\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def get_key_padding_mask(tokens):\n",
    "        \"\"\"\n",
    "        用于key_padding_mask\n",
    "        \"\"\"\n",
    "        key_padding_mask = torch.zeros(tokens.size())\n",
    "        key_padding_mask[tokens == PAD_token] = -torch.inf\n",
    "        return key_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fef46617",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PoetTaskModel(\n",
       "  (embedding): Embedding(10000, 128)\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-2): 3 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-2): 3 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (predictor): Linear(in_features=128, out_features=10000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PoetTaskModel()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "746a58f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 128])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试样本能否正常输入网络\n",
    "src = x_train_tensor[:1]\n",
    "tgt = y_train_tensor[:1]\n",
    "out = model(src, tgt)\n",
    "out.shape # torch.Size([1, 10, 10000]) 代表 batch_size, seq_len, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a32ca99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imcurie/miniconda3/envs/llm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075fdc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 4\n",
    "epochs = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for i in tqdm(range(0, len(x_train_tensor), batch_size)):\n",
    "        src = x_train_tensor[i:i+batch_size]\n",
    "        tgt = y_train_tensor[i:i+batch_size]\n",
    "        tgt_y = decoder_output_train_tensor[i:i+batch_size]\n",
    "\n",
    "        # 清空梯度\n",
    "        optimizer.zero_grad()\n",
    "        # 进行transformer的计算\n",
    "        out = model(src, tgt)\n",
    "        # 将结果送给最后的线性层进行预测\n",
    "        out = model.predictor(out)\n",
    "        # 计算loss\n",
    "        loss = criterion(out.view(-1, out.size(-1)), tgt_y.view(-1))\n",
    "        # 计算梯度\n",
    "        loss.backward()\n",
    "        # 更新参数\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(\"epoch: {}, loss: {}\".format(epoch, total_loss / len(x_train_tensor)))\n",
    "    # 计算训练集上的准确率\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        src = x_train_tensor\n",
    "        tgt = y_train_tensor\n",
    "        tgt_y = decoder_output_train_tensor\n",
    "\n",
    "        out = model(src, tgt)\n",
    "        out = model.predictor(out)\n",
    "        out = out.argmax(dim=-1)\n",
    "        acc = (out == tgt_y).float().mean()\n",
    "        print(\"train acc: {}\".format(acc))\n",
    "    # 计算测试集上的准确率\n",
    "    with torch.no_grad():\n",
    "        src = x_test_tensor\n",
    "        tgt = y_test_tensor\n",
    "        tgt_y = decoder_output_test_tensor\n",
    "\n",
    "        out = model(src, tgt)\n",
    "        out = model.predictor(out)\n",
    "        out = out.argmax(dim=-1)\n",
    "        acc = (out == tgt_y).float().mean()\n",
    "        print(\"test acc: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bb6847",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = '白日依山盡，黃河入海流。'\n",
    "\n",
    "\n",
    "# 把 test_string 每个字之间添加空格\n",
    "test_string = ' '.join(test_string)\n",
    "# 前后加上开始和结束标志\n",
    "# test_string = '<SOS> ' + test_string\n",
    "# 把 test_string 转化为token\n",
    "test_string_token = tokenizer.texts_to_sequences([test_string])\n",
    "# 截取后10个字\n",
    "test_string_token = test_string_token[0][-10:]\n",
    "# 转化为 numpy，补齐\n",
    "test_string_mat = pad_sequences([test_string_token],maxlen=10,padding='post',truncating='post')\n",
    "# test_string_mat 转化为 tensor\n",
    "test_string_tensor = torch.tensor(test_string_mat).to(device)\n",
    "test_string_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f079a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = decode(model, \n",
    "       src=test_string_tensor,  \n",
    "       max_iter=100, \n",
    "       SOS_token=SOS_token, \n",
    "       EOS_token=EOS_token, \n",
    "       PAD_token=PAD_token)\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf967a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把 tgt 中的数字转化为文字\n",
    "tokenizer.sequences_to_texts(decoded.cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
